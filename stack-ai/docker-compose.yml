services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    entrypoint: ["/bin/sh", "/entrypoint.sh"]
    command: []
    volumes:
      - ./entrypoint.sh:/entrypoint.sh:ro
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    expose:
      - "11434"
    networks:
      - default

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    environment:
      - PORT=8083
      - OLLAMA_BASE_URL=http://ollama:11434/
    ports:
      - "3003:8083"
    volumes:
      - open-webui:/app/backend/data
    restart: always
    networks:
      - default

  fooocus-api:
    # Imagen personalizada para intentar dar soporte a GPUs nuevas (sm_120 / RTX 50xx)
    build:
      context: .
      dockerfile: Dockerfile.fooocus-gpu
    ports:
      - "3004:8084"
    networks:
      - default
    # Soporte GPU (si la arquitectura no está soportada, el script interno hace fallback CPU)
    gpus: all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - FORCE_CUDA=1
      # Pasar argumentos adicionales a Fooocus (opcional)
      # - CLI_ARGS=--listen 0.0.0.0 --port 8084 --always-download-new-model
      # Forzar CPU siempre (descomenta la siguiente línea si quieres evitar intento CUDA inicial)
      # - CUDA_VISIBLE_DEVICES=
    volumes:
      # Cache modelos/pesos para evitar re-descargas entre reconstrucciones
      - fooocus-cache:/app/fooocus/models
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8084 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 120s

networks:
  default:
    name: proxy-network
    external: true

volumes:
  open-webui:
  openhands-data:
  fooocus-cache: