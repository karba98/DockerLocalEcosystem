## Dockerfile.fooocus-gpu
## Construye un contenedor para Fooocus intentando soportar GPUs recientes.
## NOTA: Si tu GPU (RTX 50xx / sm_120) aún no está soportada por PyTorch precompilado,
## tendrás que usar modo CPU (establece CUDA_VISIBLE_DEVICES="") o compilar PyTorch desde fuente.

## Imagen base: usar una etiqueta existente. Ajustamos a cu121 estable.
FROM pytorch/pytorch:2.2.2-cuda12.1-cudnn8-runtime

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 \
    PYTHONUNBUFFERED=1 \
    GRADIO_SERVER_PORT=8084 \
    HF_HUB_DISABLE_SYMLINKS_WARNING=1 \
    PYTORCH_ENABLE_MPS_FALLBACK=1 \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

WORKDIR /app

# Dependencias del sistema y clon del repositorio en una sola capa
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential git wget \
    && rm -rf /var/lib/apt/lists/* \
    && git clone https://github.com/lllyasviel/Fooocus.git fooocus \
    && echo "Repositorio Fooocus clonado"

WORKDIR /app/fooocus

# Instalar dependencias. Torch ya viene en la imagen base.
# Si el repositorio trae requirements_versions.txt lo usamos; el script launch.py volverá a validar.
RUN pip install --upgrade pip && \
    if [ -f requirements_versions.txt ]; then pip install -r requirements_versions.txt; fi && \
    pip install Pillow accelerate fastapi safetensors transformers uvicorn

# (Opcional) Forzar CPU temporal si tu GPU no está soportada todavía:
# ENV CUDA_VISIBLE_DEVICES=""

# Exponer puerto
EXPOSE 8084

# Comando real: utilizar launch.py (prepara entorno y lanza Gradio) y escuchar en 0.0.0.0
ENV CLI_ARGS="--listen 0.0.0.0 --port 8084"

# Copiar script de arranque (se añadirá al repo como start.sh)
COPY start.sh /app/start.sh
RUN chmod +x /app/start.sh

CMD ["/app/start.sh"]
