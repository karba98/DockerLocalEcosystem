# Docker Local Ecosystem

Ecosistema local de servicios Docker para desarrollo y pruebas: proxy Nginx, IA (Ollama + OpenWebUI + Fooocus opcional), SonarQube y utilidades.

---

## üó∫Ô∏è Esquema de arquitectura

```mermaid
flowchart LR
    subgraph Navegador
        A[Usuario en localhost:80]
    end
    subgraph Nginx
        B(proxy-nginx)
    end
    subgraph Stacks
        C(OpenWebUI)
        D(SonarQube)
        E(Ollama)
    end
    A-->|HTTP|B
    B-->|:3003|C
    B-->|:9000|D
    C-->|:11434|E
    B-->|/ollama|E
```

---

## Servicios incluidos

- **Nginx** (proxy-nginx): Proxy inverso y p√°gina de inicio.
- **Ollama**: Servidor de modelos LLM locales.
- **OpenWebUI**: Interfaz web para trabajar con Ollama.
- **Fooocus API (experimental)**: Generaci√≥n de im√°genes (requiere GPU; soporta fallback CPU si no hay soporte CUDA para tu arquitectura).
- **SonarQube**: An√°lisis de calidad de c√≥digo.

---

## Instalaci√≥n y arranque

1. Aseg√∫rate de tener Docker y Docker Compose instalados.
2. Clona este repositorio y sit√∫ate en la ra√≠z del proyecto.
3. Ejecuta uno de estos scripts seg√∫n tu sistema:

### En Linux/Mac/WSL
```bash
git clone https://github.com/karba98/docker-local-ecosystem.git
cd docker-local-ecosystem
bash start-ecosystem.sh
```

### En Windows (PowerShell)
```powershell
git clone https://github.com/karba98/docker-local-ecosystem.git
cd docker-local-ecosystem
./start-ecosystem.ps1
```

---

## Scripts de arranque

Ambos scripts (`start-ecosystem.sh` y `start-ecosystem.ps1`) permiten levantar selectivamente los stacks.

### PowerShell (`start-ecosystem.ps1`) par√°metros

```
    # Docker Local Ecosystem

    Ecosistema reproducible para experimentar localmente con: proxy Nginx, LLMs (Ollama + OpenWebUI), generaci√≥n de im√°genes (Fooocus), an√°lisis de calidad (SonarQube) y utilidades. Orientado a desarrollo iterativo r√°pido y pruebas sobre una √∫nica red compartida (`proxy-network`).

    ---

    ## üó∫Ô∏è Arquitectura actualizada

    ```mermaid
    flowchart LR
        subgraph Client
            U[Usuario<br/>http://localhost]
        end

        subgraph proxy-network
            subgraph Nginx
                N[proxy-nginx]
            end
            subgraph AI[stack-ai]
                OW(OpenWebUI)
                OL(Ollama)
                F(Fooocus API)
            end
            subgraph SQ[stack-sonarqube]
                S(SonarQube)
            end
        end

        U -->|HTTP 80| N
        N -->|/ollama ‚Üí 11434| OL
        N -->|/openwebui :3003| OW
        N -->|/fooocus :3004| F
        N -->|/sonarqube :9000| S
        OW -->|REST| OL
        F -. opcional .- OL
    ```

    Caracter√≠sticas clave:
    * Una sola red externa (`proxy-network`).
    * Acceso unificado v√≠a Nginx (puerto 80) a los servicios web.
    * OpenWebUI consume Ollama internamente (sin exponer 11434 p√∫blicamente si no deseas).
    * Fooocus expone API HTTP (8084 interno ‚Üí 3004 host) con fallback autom√°tico a CPU.
    * SonarQube accesible en `/sonarqube` (puerto interno 9000) si el stack est√° levantado.

    ---

    ## Stacks

    | Nombre l√≥gico | Ruta                         | Descripci√≥n |
    |---------------|------------------------------|-------------|
    | Principal      | `./`                        | Nginx + p√°gina √≠ndice est√°tica |
    | stack-ai       | `./stack-ai`                | Ollama, OpenWebUI, Fooocus (build local) |
    | stack-sonarqube| `./stack- sonarqube`        | SonarQube (server + required DB) |

    Puedes iniciar cualquiera en aislamiento o combinarlos.

    ---

    ## Servicios incluidos

    * **proxy-nginx**: Reverse proxy & landing page.
    * **Ollama**: Servidor de modelos LLM (pull din√°mico de modelos).
    * **OpenWebUI**: Interfaz rica para interactuar con Ollama.
    * **Fooocus API (experimental)**: Generaci√≥n de im√°genes / text-to-image.
    * **SonarQube**: Plataforma de calidad y seguridad de c√≥digo.

    ---

    ## Requisitos previos

    * Docker Engine + Docker Compose plugin.
    * (Opcional GPU) Drivers NVIDIA + `nvidia-container-toolkit` si vas a usar Ollama con GPU o Fooocus acelerado.
    * PowerShell 7+ en Windows (o WSL) para mejor experiencia.

    ---

    ## Instalaci√≥n r√°pida

    Clona y ejecuta el script seg√∫n tu OS:

    ### Linux / macOS / WSL
    ```bash
    git clone https://github.com/karba98/docker-local-ecosystem.git
    cd docker-local-ecosystem
    ./start-ecosystem.sh -Auto
    ```

    ### Windows (PowerShell)
    ```powershell
    git clone https://github.com/karba98/docker-local-ecosystem.git
    cd docker-local-ecosystem
    ./start-ecosystem.ps1 -Auto
    ```

    El flag `-Auto` selecciona todos los stacks detectados.

    ---

    ## Scripts de orquestaci√≥n

    Ambos scripts tienen paridad funcional. Seleccionan, construyen (si procede) y levantan stacks. Auto-actualizan el repo (`git pull --rebase`) al inicio.

    ### PowerShell (`start-ecosystem.ps1`)
    Par√°metros:
    ```
    -Stacks <lista>   Selecci√≥n no interactiva (ej: -Stacks Principal stack-ai)
    -Auto             Selecciona todos los stacks (sincroniza con -Stacks si no se pasa lista)
    -SkipBuild        No ejecuta docker compose build
    -BuildOnly        Construye im√°genes y sale sin levantar
    -NoPull           Evita --pull durante build
    -NoCache          Fuerza rebuild completo (--no-cache)
    -List             Lista stacks detectados y sale
    ```
    Ejemplos:
    ```powershell
    ./start-ecosystem.ps1                     # Modo interactivo
    ./start-ecosystem.ps1 -List               # Ver stacks
    ./start-ecosystem.ps1 -Auto               # Todos
    ./start-ecosystem.ps1 -Stacks stack-ai    # S√≥lo IA
    ./start-ecosystem.ps1 -Stacks stack-ai -BuildOnly
    ./start-ecosystem.ps1 -Stacks Principal stack-ai -NoCache -NoPull
    ./start-ecosystem.ps1 -Auto -SkipBuild
    ```

    ### Bash (`start-ecosystem.sh`)
    Par√°metros:
    ```
    -Stacks <lista>   Selecci√≥n no interactiva (Principal stack-ai stack-sonarqube All)
    -Auto             Selecciona todos
    -SkipBuild        No construye im√°genes
    -BuildOnly        S√≥lo build y salir
    -NoPull           Omite --pull
    -NoCache          Usa --no-cache
    -List             Lista stacks y sale
    ```
    Ejemplos:
    ```bash
    ./start-ecosystem.sh -List
    ./start-ecosystem.sh -Stacks stack-ai
    ./start-ecosystem.sh -Stacks Principal stack-ai -BuildOnly
    ./start-ecosystem.sh -Auto -NoCache -NoPull
    ./start-ecosystem.sh -Auto -SkipBuild
    ```

    Notas internas:
    * Orden de build: stacks secundarios ‚Üí principal (para reutilizar capas comunes y no bloquear nginx antes de tiempo).
    * `-SkipBuild` ignora `-NoCache` / `-NoPull` (ya se advierte en consola).

    ---

    ## Fooocus API (experimental)

    Ubicado en `stack-ai`. Construido desde `Dockerfile.fooocus-gpu`.

    Resumen t√©cnico:
    * Base: `nvidia/cuda:12.8.0-runtime-ubuntu22.04`.
    * Instalaci√≥n forzada de `torch==2.7.0+cu128` (soporte RTX 50xx / sm_120).
    * Fallback CPU autom√°tico si inicializaci√≥n CUDA falla (script `start.sh`).
    * Volumen persistente `fooocus-cache` para modelos/pesos.
    * Puerto interno 8084 ‚Üí host `http://localhost:3004`.

    Build Args (en `stack-ai/docker-compose.yml`):
    | Arg | Valor por defecto | Descripci√≥n |
    |-----|-------------------|-------------|
    | `INSTALL_TOOLKIT` | `0` | Si `1`, instala toolkit completo (compiladores / headers). M√°s lento. |
    | `TORCH_FORCE_VERSION` | `2.7.0+cu128` | Versi√≥n exacta que se reinstala tras clonar Fooocus. |

    Para activar toolkit y reconstruir s√≥lo Fooocus:
    ```powershell
    cd stack-ai
    docker compose build --no-cache fooocus-api
    docker compose up -d fooocus-api
    ```

    Pasar argumentos a Fooocus: descomenta `CLI_ARGS` en la secci√≥n `environment` del servicio y a√±ade flags (ej: `--always-download-new-model`).

    Forzar CPU permanente: descomenta `CUDA_VISIBLE_DEVICES=` (valor vac√≠o) en `environment`.

    Logs en tiempo real:
    ```powershell
    docker logs -f fooocus-api
    ```

    ### Problemas comunes Fooocus
    | S√≠ntoma | Causa probable | Acci√≥n |
    |---------|----------------|--------|
    | Mensaje sobre arquitectura no soportada | GPU muy reciente (sm_120) | Esperar fallback CPU o instalar toolkit y reconstruir |
    | OOM / memoria insuficiente | Modelos demasiado grandes | Reducir resoluci√≥n, batch o usar CPU temporalmente |
    | Descargas lentas | Red o sin cache | Verificar volumen `fooocus-cache` montado |

    ---

    ## SonarQube

    Stack opcional `stack-sonarqube`. Puedes a√±adir un archivo `sonar-project.properties` en la ra√≠z de tu proyecto a analizar y lanzar el scanner localmente apuntando a la URL de SonarQube dentro de la red o v√≠a proxy.

    Ejemplo m√≠nimo de `sonar-project.properties`:
    ```
    sonar.projectKey=demo
    sonar.projectName=Demo
    sonar.sources=.
    sonar.sourceEncoding=UTF-8
    ```

    Accede a la interfaz: `http://localhost/sonarqube` (o puerto directo 9000 si lo expones).

    ---

    ## Vol√∫menes y persistencia

    | Volumen | Servicio | Contenido |
    |---------|----------|-----------|
    | `open-webui` | OpenWebUI | Datos / historial / configuraci√≥n |
    | `fooocus-cache` | Fooocus | Modelos y pesos descargados |

    Puedes listar vol√∫menes: `docker volume ls`.

    ---

    ## Red compartida

    Se crea autom√°ticamente `proxy-network` si no existe. Es externa, de modo que puedes acoplar otros stacks propios usando:
    ```bash
    docker network connect proxy-network <otro-contenedor>
    ```

    ---

    ## Troubleshooting r√°pido

    | Problema | Diagn√≥stico r√°pido | Soluci√≥n |
    |----------|--------------------|----------|
    | Puerto 80 en uso | `netstat -ano | find "0.0.0.0:80"` | Libera IIS / otro proxy / cambia puerto en `docker-compose.yml` principal |
    | GPU no detectada | `docker run --rm --gpus all nvidia/cuda:12.8.0-runtime-ubuntu22.04 nvidia-smi` | Instala / reconfigura `nvidia-container-toolkit` |
    | Fooocus lento | Fallback CPU | Activa GPU corrigiendo drivers o ajusta INSTALL_TOOLKIT=1 y rebuild |
    | OpenWebUI no ve modelos | Ollama no arranc√≥ a√∫n | Revisa `docker logs ollama` |
    | SonarQube tarda en iniciar | Inicializaci√≥n DB | Esperar (primer arranque puede ser 2-3 min) |

    Ver logs de todos:
    ```bash
    docker ps --format 'table {{.Names}}\t{{.Status}}'
    docker logs -f <nombre-contenedor>
    ```

    ---

    ## Actualizaciones

    Los scripts ejecutan `git pull --rebase` al inicio. Si tienes cambios locales que no quieres sobrescribir, realiza un commit antes de lanzar el script.

    ---

    ## Estructura del repositorio
    ```text
    ‚îú‚îÄ‚îÄ docker-compose.yml              # Stack principal (proxy + landing)
    ‚îú‚îÄ‚îÄ stack-ai/
    ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml          # Ollama, OpenWebUI, Fooocus
    ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.fooocus-gpu      # Build CUDA / Torch personalizado
    ‚îÇ   ‚îú‚îÄ‚îÄ entrypoint.sh               # Entrypoint para Ollama
    ‚îÇ   ‚îî‚îÄ‚îÄ start.sh                    # Script arranque Fooocus (fallback CPU)
    ‚îú‚îÄ‚îÄ stack- sonarqube/
    ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml          # SonarQube
    ‚îú‚îÄ‚îÄ static/                         # Recursos est√°ticos web
    ‚îú‚îÄ‚îÄ index.html                      # Landing
    ‚îú‚îÄ‚îÄ nginx.conf                      # Config proxy
    ‚îú‚îÄ‚îÄ start-ecosystem.sh              # Script Bash
    ‚îú‚îÄ‚îÄ start-ecosystem.ps1             # Script PowerShell
    ‚îî‚îÄ‚îÄ README.md
    ```

    ---

    ## Roadmap (ideas futuras)
    * Output JSON opcional para scripts (-Json) -> integraci√≥n CI.
    * M√©tricas b√°sicas (tiempos de build) almacenadas en un archivo temporal.
    * Endpoint de salud consolidado en Nginx (/health) agregando checks de servicios.
    * Integraci√≥n opcional de vector store / RAG.

    ---

    ## Cr√©ditos
    * [OpenWebUI](https://github.com/open-webui/open-webui)
    * [Ollama](https://github.com/jmorganca/ollama)
    * [Fooocus](https://github.com/lllyasviel/Fooocus)
    * [SonarQube](https://www.sonarqube.org/)
    * [Bootstrap](https://getbootstrap.com/)

    ---

    Contribuciones y sugerencias bienvenidas. Crea un issue o PR. ‚≠ê